{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading pretrained model to /Users/mask_rcnn_coco.h5 ...\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/Users/mask_rcnn_coco.h5'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0a6977131feb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Download COCO trained weights from Releases if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOCO_MODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_trained_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOCO_MODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Mask_RCNN/mrcnn/utils.py\u001b[0m in \u001b[0;36mdownload_trained_weights\u001b[0;34m(coco_model_path, verbose)\u001b[0m\n\u001b[1;32m    845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading pretrained model to \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcoco_model_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOCO_MODEL_URL\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoco_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/Users/mask_rcnn_coco.h5'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nConfigurations:\nBACKBONE                       resnet101\nBACKBONE_STRIDES               [4, 8, 16, 32, 64]\nBATCH_SIZE                     8\nBBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\nCOMPUTE_BACKBONE_SHAPE         None\nDETECTION_MAX_INSTANCES        100\nDETECTION_MIN_CONFIDENCE       0.7\nDETECTION_NMS_THRESHOLD        0.3\nFPN_CLASSIF_FC_LAYERS_SIZE     1024\nGPU_COUNT                      1\nGRADIENT_CLIP_NORM             5.0\nIMAGES_PER_GPU                 8\nIMAGE_CHANNEL_COUNT            3\nIMAGE_MAX_DIM                  128\nIMAGE_META_SIZE                16\nIMAGE_MIN_DIM                  128\nIMAGE_MIN_SCALE                0\nIMAGE_RESIZE_MODE              square\nIMAGE_SHAPE                    [128 128   3]\nLEARNING_MOMENTUM              0.9\nLEARNING_RATE                  0.001\nLOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\nMASK_POOL_SIZE                 14\nMASK_SHAPE                     [28, 28]\nMAX_GT_INSTANCES               100\nMEAN_PIXEL                     [123.7 116.8 103.9]\nMINI_MASK_SHAPE                (56, 56)\nNAME                           shapes\nNUM_CLASSES                    4\nPOOL_SIZE                      7\nPOST_NMS_ROIS_INFERENCE        1000\nPOST_NMS_ROIS_TRAINING         2000\nPRE_NMS_LIMIT                  6000\nROI_POSITIVE_RATIO             0.33\nRPN_ANCHOR_RATIOS              [0.5, 1, 2]\nRPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\nRPN_ANCHOR_STRIDE              1\nRPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\nRPN_NMS_THRESHOLD              0.7\nRPN_TRAIN_ANCHORS_PER_IMAGE    256\nSTEPS_PER_EPOCH                100\nTOP_DOWN_PYRAMID_SIZE          256\nTRAIN_BN                       False\nTRAIN_ROIS_PER_IMAGE           32\nUSE_MINI_MASK                  True\nUSE_RPN_ROIS                   True\nVALIDATION_STEPS               5\nWEIGHT_DECAY                   0.0001\n\n\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7928c4edfc77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create model in training mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model = modellib.MaskRCNN(mode=\"training\", config=config,\n\u001b[0;32m----> 3\u001b[0;31m                           model_dir=MODEL_DIR)\n\u001b[0m",
      "\u001b[0;32m~/Mask_RCNN/mrcnn/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mode, config, model_dir)\u001b[0m\n\u001b[1;32m   1835\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1836\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_log_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1837\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1839\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Mask_RCNN/mrcnn/model.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, mode, config)\u001b[0m\n\u001b[1;32m   1854\u001b[0m         \u001b[0;31m# Inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m         input_image = KL.Input(\n\u001b[0;32m-> 1856\u001b[0;31m             shape=[None, None, config.IMAGE_SHAPE[2]], name=\"input_image\")\n\u001b[0m\u001b[1;32m   1857\u001b[0m         input_image_meta = KL.Input(shape=[config.IMAGE_META_SIZE],\n\u001b[1;32m   1858\u001b[0m                                     name=\"input_image_meta\")\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[0;34m(shape, batch_shape, name, dtype, sparse, tensor)\u001b[0m\n\u001b[1;32m    176\u001b[0m                              \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                              \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                              input_tensor=tensor)\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0;31m# Return tensor including _keras_shape and _keras_history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# Note that in this case train_output and test_output are the same pointer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, batch_size, batch_input_shape, dtype, input_tensor, sparse, name)\u001b[0m\n\u001b[1;32m     85\u001b[0m                                          \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                                          \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                                          name=self.name)\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(shape, ndim, dtype, sparse, name)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_placeholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_learning_phase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b502c6c49fa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# are different due to the different number of classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# See README for instructions to download the COCO weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     model.load_weights(COCO_MODEL_PATH, by_name=True,\n\u001b[0m\u001b[1;32m     11\u001b[0m                        exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n\u001b[1;32m     12\u001b[0m                                 \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-83fb3ae74319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# layers. You can also pass a regular expression to select\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# which layers to train by name pattern.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model.train(dataset_train, dataset_val, \n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9f8e5038d063>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# pass a regular expression to select which layers to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# train by name pattern.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model.train(dataset_train, dataset_val, \n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-53b45c7b3d24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m model = modellib.MaskRCNN(mode=\"inference\", \n\u001b[1;32m      9\u001b[0m                           \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minference_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                           model_dir=MODEL_DIR)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Get path to saved weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Mask_RCNN/mrcnn/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mode, config, model_dir)\u001b[0m\n\u001b[1;32m   1835\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1836\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_log_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1837\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1839\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Mask_RCNN/mrcnn/model.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, mode, config)\u001b[0m\n\u001b[1;32m   1854\u001b[0m         \u001b[0;31m# Inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m         input_image = KL.Input(\n\u001b[0;32m-> 1856\u001b[0;31m             shape=[None, None, config.IMAGE_SHAPE[2]], name=\"input_image\")\n\u001b[0m\u001b[1;32m   1857\u001b[0m         input_image_meta = KL.Input(shape=[config.IMAGE_META_SIZE],\n\u001b[1;32m   1858\u001b[0m                                     name=\"input_image_meta\")\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[0;34m(shape, batch_shape, name, dtype, sparse, tensor)\u001b[0m\n\u001b[1;32m    176\u001b[0m                              \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                              \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                              input_tensor=tensor)\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0;31m# Return tensor including _keras_shape and _keras_history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# Note that in this case train_output and test_output are the same pointer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, batch_size, batch_input_shape, dtype, input_tensor, sparse, name)\u001b[0m\n\u001b[1;32m     85\u001b[0m                                          \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                                          \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                                          name=self.name)\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(shape, ndim, dtype, sparse, name)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_placeholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_learning_phase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_image           shape: (128, 128, 3)         min:   90.00000  max:  193.00000  uint8\nimage_meta               shape: (16,)                 min:    0.00000  max:  128.00000  int64\ngt_class_id              shape: (1,)                  min:    2.00000  max:    2.00000  int32\ngt_bbox                  shape: (1, 4)                min:    6.00000  max:  122.00000  int32\ngt_mask                  shape: (128, 128, 1)         min:    0.00000  max:    1.00000  bool\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-29454f7a88ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moriginal_image\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n\u001b[1;32m      5\u001b[0m                             dataset_val.class_names, r['scores'], ax=get_ax())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1202a181ecc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmolded_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodellib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmold_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Run object detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Compute AP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
